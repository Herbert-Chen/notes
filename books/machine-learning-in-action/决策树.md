#决策树
##简介
判断模块，引出分支；终止模块

对比KNN，KNN无法给出数据的内在含义。

优：
* 计算复杂度不高
* 输出结果易于理解
* 允许缺失中间数据
* 可以处理不相关特征数据

缺：
容易过拟合

##决策树构造
###构造思路
* 多特征，选哪一个？影响最大的
* 划分后，是否同一类？是否继续？

思路：
	Check if every item in the dataset is in the same class:
		If so return the class label
		Else
			find the best feature to split the data
			split the dataset
			create a branch node
				for each split
					call createBranch and add the result to the branch node
			return branch node

#####原则，度量方式
原则：将无序的数据变得更加有序，考虑增量。

信息度量方式：
* 熵，信息的期望值
* 基尼不纯度，随机选取，被错误分类的概率



每一类信息的定义：

![3.1](3.1.png)

熵：

![3.2](3.2.png)

python实现：

	from math import log
	def calcShannonEnt(dataSet):
    	numEntries = len(dataSet)
    	labelCounts = {}
    	for featVec in dataSet: #the the number of unique elements and their occurance
    	    currentLabel = featVec[-1]
    	    if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0
    	    labelCounts[currentLabel] += 1
    	shannonEnt = 0.0
    	for key in labelCounts:
    	    prob = float(labelCounts[key])/numEntries
    	    shannonEnt -= prob * log(prob,2) #log base 2
    	return shannonEnt

#####按特征划分
特征对应一维，按维划分
某个特征等于某个值，剔除特征，剩余数据保留，划分出来
	注意，append添加，extend扩展

	def splitDataSet(dataSet, axis, value):
    	retDataSet = []
    	for featVec in dataSet:
    	    if featVec[axis] == value:
    	        reducedFeatVec = featVec[:axis]     #chop out axis used for splitting
    	        reducedFeatVec.extend(featVec[axis+1:])
    	        retDataSet.append(reducedFeatVec)
    	return retDataSet


选择最好的特征：

	def chooseBestFeatureToSplit(dataSet):
	    numFeatures = len(dataSet[0]) - 1      #the last column is used for the labels，行向量
	    baseEntropy = calcShannonEnt(dataSet)
	    bestInfoGain = 0.0; bestFeature = -1
	    for i in range(numFeatures):        #iterate over all the features
	        featList = [example[i] for example in dataSet]#create a list of all the examples of this																	feature
	        uniqueVals = set(featList)       #get a set of unique values
	        newEntropy = 0.0
	        for value in uniqueVals:
	            subDataSet = splitDataSet(dataSet, i, value)
	            prob = len(subDataSet)/float(len(dataSet))
	            newEntropy += prob * calcShannonEnt(subDataSet)     
	        infoGain = baseEntropy - newEntropy     #calculate the info gain; ie reduction in entropy
	        if (infoGain > bestInfoGain):       #compare this to the best gain so far
	            bestInfoGain = infoGain         #if better than current best, set to best
	            bestFeature = i
	    return bestFeature                      #returns an integer

	先选特征，再选值！！！

#####递归构建
终止条件：
* 遍历完所有属性
* 每个分支下只有一个实例

注意，第一个条件，并不是每次划分时特征数目都减少；已处理完所有特性但标签仍不唯一，多数表决。


多数表决程序（字典）：   

	def majorityCnt(classList):
    	classCount={}
    	for vote in classList:
        	if vote not in classCount.keys(): classCount[vote] = 0
        	classCount[vote] += 1
    	sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)
    	return sortedClassCount[0][0]
	
创建树的代码：
	
	def createTree(dataSet,labels):
    	classList = [example[-1] for example in dataSet]
    	if classList.count(classList[0]) == len(classList): 
        	return classList[0]				#stop splitting when all of the classes are equal
    	if len(dataSet[0]) == 1: 			#stop splitting when there are no more features in dataSet
    	    return majorityCnt(classList)
    	bestFeat = chooseBestFeatureToSplit(dataSet)
    	bestFeatLabel = labels[bestFeat]
    	myTree = {bestFeatLabel:{}}
    	del(labels[bestFeat])
    	featValues = [example[bestFeat] for example in dataSet]
    	uniqueVals = set(featValues)
    	for value in uniqueVals:
        	subLabels = labels[:]       #copy all of labels, so trees don't mess up existing labels
        	myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)
    	return myTree               

	检验list内元素都一样；
	















































